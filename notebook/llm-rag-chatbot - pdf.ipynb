{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0638a64-1af7-46fa-9e73-7cf5cf501da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51c5415-d4a5-4e1d-901b-955652771e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "def load_pdf(path):\n",
    "    pages = []\n",
    "    doc = fitz.open(path)\n",
    "    for i in range(len(doc)):\n",
    "        page = doc[i]\n",
    "        text = page.get_text(\"text\")  # \"text\" preserves natural reading order\n",
    "        if text.strip():  # skip empty pages\n",
    "            pages.append({\"page\": i + 1, \"text\": text})\n",
    "    doc.close()\n",
    "    return pages\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0839df18-ca47-4c23-a124-7d279d0abc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split PDF into 7 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your knowledge base file (relative to the notebook)\n",
    "knowledge_base_path = '../data/path.pdf'\n",
    "\n",
    "# Load the document content from PDF (returns list of dicts with page + text)\n",
    "pages = load_pdf(knowledge_base_path)\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "# Split each page into chunks and keep page metadata\n",
    "documents = []\n",
    "for page in pages:\n",
    "    chunks = text_splitter.create_documents([page[\"text\"]])\n",
    "    # add metadata (page number) to each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata = {\"page\": page[\"page\"]}\n",
    "    documents.extend(chunks)\n",
    "\n",
    "print(f\"Split PDF into {len(documents)} chunks.\")\n",
    "# print(documents[0].page_content)  # Optional: inspect first chunk\n",
    "# print(documents[0].metadata)      # Optional: check page number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f95ae0-5c9a-4a33-9c9b-721f0aa654eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milee\\AppData\\Local\\Temp\\ipykernel_3228\\3922411563.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS vector store created.\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings model\n",
    "# This model converts text into dense vector representations\n",
    "embeddings_model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # A good balance of size and performance\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
    "\n",
    "# Create a FAISS vector store from the document chunks and embeddings\n",
    "# This step creates an index that allows for efficient similarity search\n",
    "vector_store = FAISS.from_documents(documents, embeddings)\n",
    "print(\"FAISS vector store created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ccdd0a9-1978-4e58-89f8-9500fa84f4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: {'What are the ways to fix the assistant?'}\n",
      "\n",
      "--- Top 3 Retrieved Documents (Text Content) ---\n",
      "Document 1:\n",
      "SKILLS \n",
      "• Technical Skills: Python, TensorFlow, Keras, PyTorch, Hugging Face, API, React, Java, C/C++, C#, PostgreSQL, Model \n",
      "Evaluation, Enhancing Model Performance, Model Integration, Automatisation, Deep Learning Frameworks, LOps, Linux Systems, \n",
      "Version Control \n",
      "• AI/ML Skills: AI/machine Learning Concepts, AI Frameworks, LLM, NLP, Statistics, Linear Algebra, Multimodal Models, Rein- \n",
      "forcement Learning, Computer Vision, Research Publication \n",
      "• Soft Skills: Analytical Skills, Curiosity, Communication \n",
      " \n",
      "CERTIFICATIONS \n",
      "•Machine Learning Concept: Google \n",
      "•Generative AI Explained: Nvidia \n",
      "•Crash Course on Python: Google \n",
      "•C for EveryOne: University of California, Santa Cruz \n",
      " \n",
      "VOLUNTEER \n",
      "P1 Games \n",
      "Oct 2024 - Mar 2025 \n",
      "AI Audio Systems Engineer (Video Game) \n",
      "• Developed AI-driven audio triggers that generated real-time contextual sound responses based on game states, player \n",
      "interactions, and events.\n",
      "--------------------------------------------------\n",
      "Document 2:\n",
      "the precision and quality of virtual reality experiences.  \n",
      "EL-EVENTS \n",
      "Feb 2024 - May 2024 \n",
      "ML / Deep Learning Engineer (Internship)                                                                                                                              Paris, France \n",
      "Developed an AI solution for real-time audio signal processing using deep learning methods, aligning iterative experimentation with \n",
      "potential research contributions.  \n",
      "• Engineered a Wave-U-Net model for audio enhancement, achieving a 10 dB improvement in signal-to-noise ratio and a PESQ score \n",
      "of 2.32.  \n",
      "• Collected, cleansed, and preprocessed raw audio data to support effective training and higher model precision. \n",
      "• Integrated the AI model into production workflows in collaboration with engineering teams, streamlining audio signal restoration \n",
      "processes.  \n",
      " \n",
      "PERSONAL PROJECT \n",
      "AI Agent and Automation for Business Analysis\n",
      "--------------------------------------------------\n",
      "Document 3:\n",
      "Faculty of Sciences Monastir, Tunisie \n",
      "Bachelor Degree, Software Engineering \n",
      "•GPA: 3.5 \n",
      "•Achievements: Specializing in software development and algorithmic foundations, with solid experience in programming and  \n",
      "•database systems. \n",
      " \n",
      "WORK EXPERIENCE \n",
      "Mercor AI \n",
      "Aug 2025 - Present \n",
      "Multimodal Model Trainer                                     \n",
      "San Francisco, California (Remote) \n",
      "Contributed to a multimodal AI research project by enriching and validating datasets with essential image-audio pairings to support \n",
      "robust model training.  \n",
      "• Produced clear and detailed audio descriptions for extensive image sets, enhancing dataset quality for multimodal deep learning \n",
      "experiments.  \n",
      "• Collaborated with research teams to assess and optimize data quality using statistical methods, which improved model training \n",
      "outcomes.  \n",
      "• Followed stringent guidelines to generate high-fidelity recordings, reinforcing the standards for training AI models. \n",
      "Carthage Studio \n",
      "Dec 2023 - Mar 2025\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define a sample query\n",
    "query_to_test_vector_store = \"What are the ways to fix the assistant?\"\n",
    "\n",
    "# Perform a similarity search\n",
    "# This will return the most semantically similar document chunks to your query\n",
    "retrieved_docs = vector_store.similarity_search(query_to_test_vector_store, k=3) # k=3 to get top 3 results\n",
    "\n",
    "print(\"Query: {}\\n\".format({query_to_test_vector_store}))\n",
    "print(\"--- Top 3 Retrieved Documents (Text Content) ---\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(doc.page_content) # This shows the original text chunk\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074b9720-27b0-450a-a36c-0fc9b18fed22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM pipeline loaded: google/flan-t5-small\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "# Define the LLM model to use from Hugging Face\n",
    "llm_model_name = \"google/flan-t5-small\" # A relatively small LLM for quick local setup\n",
    "\n",
    "# Load the tokenizer for the LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "# Load the LLM model itself\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(llm_model_name)\n",
    "\n",
    "# Create a Hugging Face pipeline for text generation\n",
    "# This simplifies using the LLM for generation tasks\n",
    "llm_pipeline = pipeline(\n",
    "    \"text2text-generation\", # Use 'text2text-generation' for Flan-T5\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200, # Max length of generated response\n",
    "    temperature=0.7,    # Controls randomness (lower = more deterministic)\n",
    "    do_sample=True,     # Enable sampling\n",
    "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available, else CPU\n",
    ")\n",
    "print(f\"LLM pipeline loaded: {llm_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34c96330-d9e3-4056-90ca-3057c3280418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG QA Chain created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milee\\AppData\\Local\\Temp\\ipykernel_3228\\1888297449.py:18: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  hf_llm = HuggingFacePipeline(pipeline=llm_pipeline)\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "prompt_template = \"\"\"\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "hf_llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=hf_llm,\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "print(\"RAG QA Chain created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238a747b-3e6b-4da2-8b5b-a4e0d5082efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot ready! Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  What is the professional summary of Mohamed Elkattoufi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\milee\\AppData\\Local\\Temp\\ipykernel_3228\\21724211.py:6: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (795 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User: What is the professional summary of Mohamed Elkattoufi\n",
      "Assistant: EDUCATION Faculty of Sciences Monastir, Tunisie Sep 2024 - Present Master, Automated Reasoning Systems and Artificial Intelligence •GPA: 2.8 •Achievements: Specializing in developing scalable system architectures, applying machine learning to real-world problems, designing multi-agent models, and advancing expertise in computer vision, probabilistic modeling, and decision-making systems. WORK EXPERIENCE Mercor AI Aug 2025 - Present Multimodal Model Trainer San Francisco, California (Remote) Contributed to a multimodal AI research project by enriching and validating datasets with essential image-audio pairings to support robust model training. • Produced clear and detailed audio descriptions for extensive image sets, enhancing dataset quality for multimodal deep learning experiments. • Collaborated with research teams to assess and optimize data quality using statistical methods, which improved model training outcomes. •\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    }
   ],
   "source": [
    "# Chat history will be used for multi-turn conversations (though simple here)\n",
    "chat_history = []\n",
    "\n",
    "def chat_with_llm(query):\n",
    "    # Pass the query and chat_history to the QA chain\n",
    "    result = qa_chain({\"question\": query, \"chat_history\": chat_history})\n",
    "\n",
    "    # Update chat history (simple memory for now)\n",
    "    chat_history.append((query, result[\"answer\"]))\n",
    "\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    print(f\"Assistant: {result['answer']}\")\n",
    "    # Optionally, print source documents to see what the LLM used\n",
    "    # print(\"\\n--- Source Documents ---\")\n",
    "    # for doc in result[\"source_documents\"]:\n",
    "    #     print(doc.page_content)\n",
    "    #     print(\"-\" * 20)\n",
    "    return result['answer']\n",
    "\n",
    "# --- Test the Chatbot ---\n",
    "print(\"Chatbot ready! Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'exit':\n",
    "        break\n",
    "    chat_with_llm(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd41466-6c1a-4c93-a0d4-b4cc8756d7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
